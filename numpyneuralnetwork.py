# -*- coding: utf-8 -*-
"""NumPyNeuralNetwork.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f6D_7EIrNcPtSsfRu-whSgv9pg0YqbXj

# Layers
"""

import numpy as np
from tqdm import tqdm

random_seed = 2764763
np.random.seed(random_seed)

class InputLayer():
    def __init__(self, input_dim):
        self.input_dim = input_dim
        self.shape = (self.input_dim,)
        
    def forward(self, input_data):
        self.input = input_data
        self.output = self.input
        return self.output

class DenseLayer():
    def __init__(self, input_dim, output_dim, activation, activation_grad):
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.shape = (self.input_dim, self.output_dim)

        self.weights = np.random.rand(input_dim, output_dim) - 0.5
        self.grad = np.zeros_like(self.weights)

        self.activation = activation
        self.activation_grad = activation_grad

        self.is_output = False

    def forward(self, input_data):
        self.input = input_data

        # Multiply the weights and the input
        # Let z be the unactivated output (without the bias)
        self.z = np.matmul(np.transpose(self.weights), self.input)
        # Apply the activation
        self.output = self.activation(self.z)
        # Add the bias
        if self.is_output == False:
            self.output = np.concatenate([np.array([1.0], ndmin=2), self.output], axis=0)
        return self.output

    def backward(self, input_data, weights_data):
        self.delta = input_data

        if self.is_output == True:
            # Just make delta the error if final layer
            self.delta = self.output - self.delta

        else:
            # Matrix multiplication
            self.delta = np.matmul(weights_data, self.delta)
            # Remove the bias
            self.delta = np.delete(self.delta, 0, axis=0)
            # "Undo" the activation
            self.delta = self.delta * self.activation_grad(self.z)

class NeuralNetwork():
    def __init__(self, loss, learning_rate):
        self.layers = []
        self.trained = False

        self.loss = loss

        self.learning_rate = learning_rate

    def add(self, layer):
        if len(self.layers) > 0:
            self.layers[-1].is_output = False
        self.layers.append(layer)
        self.layers[-1].is_output = True

    def predict(self, X):
        if self.trained == False:
            print("Model not trained!")
        else:
            self.X_pred = X
            self.y_pred = []

            num_examples = X.shape[0]

            for i in range(num_examples):
                x = np.expand_dims(np.transpose(X[i, :]), axis=1)
                h = x.copy()

                for layer in self.layers:
                    h = layer.forward(h)

                self.y_pred.append(h)

            self.y_pred = np.array(self.y_pred, ndmin=2)

            return self.y_pred
            

    def fit(self, X, y, val_data=None, batch_size=32, epochs=10):
        self.X = X
        self.y = y
        self.val_data = val_data
        self.batch_size = batch_size
        self.epochs = epochs
        self.history = {'loss': [], 'val_loss': []}

        num_examples = X.shape[0]

        for epoch in range(self.epochs):
            total_loss = 0
            
            for i in range(num_examples):
                x = np.expand_dims(np.transpose(X[i, :]), axis=1)

                # Feed forward
                h = x.copy()
                for layer in self.layers:
                    h = layer.forward(h)
                
                training_loss = self.loss(self.y[i], h)
                total_loss += training_loss

                # Backpropagation
                # Accumulate layer gradients
                for l in reversed(range(len(self.layers))):
                    
                    if l > 0:
                        if self.layers[l].is_output == True:
                            self.layers[l].backward(self.y[i], weights_data=None)

                        else:
                            self.layers[l].backward(self.layers[l + 1].delta, weights_data=self.layers[l + 1].weights)

                        self.layers[l].grad += np.matmul(self.layers[l - 1].output, np.transpose(self.layers[l].delta))

            # Get average and update weights
            for l in reversed(range(len(self.layers))):
                    if l > 0:
                        self.layers[l].grad /= num_examples
                        self.layers[l].weights -= self.learning_rate * self.layers[l].grad

            mean_loss = total_loss / num_examples

            if self.val_data == None:
                print("Epoch", epoch)
                print("Training Loss:", mean_loss)
            else:
                val_X = self.val_data[0]
                val_y = self.val_data[1]
                num_val_examples = val_X.shape[0]
                total_val_loss = 0
                for i in range(num_val_examples):
                    val_x = np.expand_dims(np.transpose(val_X[i, :]), axis=1)
                    val_h = val_x.copy()
                    for layer in self.layers:
                        val_h = layer.forward(val_h)
                    val_loss = self.loss(val_y[i], h)
                    total_val_loss += val_loss
                mean_val_loss = total_val_loss / num_val_examples

                print("Epoch", epoch)
                print("Training Loss:", mean_loss, "  Validation Loss:", mean_val_loss)
                self.history['loss'].append(mean_loss)
                self.history['val_loss'].append(mean_val_loss)

        self.trained = True
        
        self.history['loss'] = np.squeeze(self.history['loss'])
        self.history['val_loss'] = np.squeeze(self.history['val_loss'])
        
        return self.history

"""# Losses"""

# y is true label value, h is prediction
def binary_crossentropy(y, h):
    return y * (-np.log(h)) + (1 - y) * (-np.log(1 - h))

"""# Activations"""

def relu(z):
    return z * (z > 0)

# Cleaned up the relu gradient
# Nice explanation of why its value at 0 doesn't matter:
# https://www.quora.com/How-do-we-compute-the-gradient-of-a-ReLU-for-backpropagation
def relu_grad(z):
    return np.array(z > 0, dtype=np.int)

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Should be (sigmoid(z)) * (1 - sigmoid(z)), rather than (sigmoid(z)) * (sigmoid(1 - z))
def sigmoid_grad(z):
    # return sigmoid(z) * sigmoid(1-z)
    return sigmoid(z) * (1 - sigmoid(z))

def tanh(z):
    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)

def tanh_grad(z):
    return 1 - (tanh(z) ** 2)

"""# Processing Data"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

data = pd.read_csv('/content/indian_liver_patient.csv')
original_data = data.copy()

"""## Missing Values

## Encoding
"""

def binary_encode(df, column, positive_value):
    df = df.copy()
    df[column] = df[column].apply(lambda x: 1 if x == positive_value else 0)
    return df

data['Albumin_and_Globulin_Ratio'] = data['Albumin_and_Globulin_Ratio'].fillna(data['Albumin_and_Globulin_Ratio'].mean())

data = binary_encode(data, 'Gender', 'Male')

"""Let's change the labels to 0, 1 instead of 1, 2"""

data = binary_encode(data, 'Dataset', 1)

data

y = data['Dataset'].copy()
X = data.drop('Dataset', axis=1).copy()

scaler = StandardScaler()
X = scaler.fit_transform(X)

y = np.expand_dims(np.array(y), axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)

"""# Training"""

X_train.shape

y_train.shape

model = NeuralNetwork(loss=binary_crossentropy, learning_rate=0.01)

model.add(InputLayer(input_dim=10))
model.add(DenseLayer(input_dim=10, output_dim=64, activation=relu, activation_grad=relu_grad))
model.add(DenseLayer(input_dim=65, output_dim=64, activation=relu, activation_grad=relu_grad))
model.add(DenseLayer(input_dim=65, output_dim=1, activation=sigmoid, activation_grad=sigmoid_grad))

for layer in model.layers:
    try:
        print(layer.weights.shape)
        print(layer.is_output)
    except:
        pass

val_data = (X_val, y_val)

batch_size=32
epochs = 200

history = model.fit(X_train, y_train, val_data=val_data, batch_size=batch_size, epochs=epochs)

np.argmin(history['val_loss'])

epochs_range = range(epochs)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 10))
plt.plot(
    epochs_range,
    history['loss'],
    label="Training Loss"
)

plt.plot(
    epochs_range,
    history['val_loss'],
)

plt.xlabel("Epoch")
plt.ylabel("Loss")

plt.legend()
plt.title("Loss Over Time")

plt.show()

np.argmin(history['val_loss'])

y_pred = model.predict(X_test)
y_pred = np.array(y_pred > 0.5, dtype=np.int)

y_pred = np.squeeze(y_pred)
y_true = np.squeeze(y_test)

# Getting accuracy

results = (y_true == y_pred)
print(np.mean(results))