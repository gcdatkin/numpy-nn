{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "data-science-experiments",
      "language": "python",
      "name": "data-science-experiments"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NumPyNeuralNetwork.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ata0xdCOX1d0"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2JOjPw9dD5E"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grgefhnrlzmT"
      },
      "source": [
        "random_seed = 2764763\n",
        "np.random.seed(random_seed)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFh6agFKdufV"
      },
      "source": [
        "class InputLayer():\n",
        "    def __init__(self, input_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.shape = (self.input_dim,)\n",
        "        \n",
        "    def forward(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = self.input\n",
        "        return self.output"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFdVTrjah5xf"
      },
      "source": [
        "class DenseLayer():\n",
        "    def __init__(self, input_dim, output_dim, activation, activation_grad):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.shape = (self.input_dim, self.output_dim)\n",
        "\n",
        "        self.weights = np.random.rand(input_dim, output_dim) - 0.5\n",
        "        self.grad = np.zeros_like(self.weights)\n",
        "\n",
        "        self.activation = activation\n",
        "        self.activation_grad = activation_grad\n",
        "\n",
        "        self.is_output = False\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input = input_data\n",
        "\n",
        "        # Multiply the weights and the input\n",
        "        # Let z be the unactivated output (without the bias)\n",
        "        self.z = np.matmul(np.transpose(self.weights), self.input)\n",
        "        # Apply the activation\n",
        "        self.output = self.activation(self.z)\n",
        "        # Add the bias\n",
        "        if self.is_output == False:\n",
        "            self.output = np.concatenate([np.array([1.0], ndmin=2), self.output], axis=0)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, input_data, weights_data):\n",
        "        self.delta = input_data\n",
        "\n",
        "        if self.is_output == True:\n",
        "            # Just make delta the error if final layer\n",
        "            self.delta = self.output - self.delta\n",
        "\n",
        "        else:\n",
        "            # Matrix multiplication\n",
        "            self.delta = np.matmul(weights_data, self.delta)\n",
        "            # Remove the bias\n",
        "            self.delta = np.delete(self.delta, 0, axis=0)\n",
        "            # \"Undo\" the activation\n",
        "            self.delta = self.delta * self.activation_grad(self.z)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7_LEk2KyjrN"
      },
      "source": [
        "class NeuralNetwork():\n",
        "    def __init__(self, loss, learning_rate):\n",
        "        self.layers = []\n",
        "        self.trained = False\n",
        "\n",
        "        self.loss = loss\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def add(self, layer):\n",
        "        if len(self.layers) > 0:\n",
        "            self.layers[-1].is_output = False\n",
        "        self.layers.append(layer)\n",
        "        self.layers[-1].is_output = True\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.trained == False:\n",
        "            print(\"Model not trained!\")\n",
        "        else:\n",
        "            self.X_pred = X\n",
        "            self.y_pred = []\n",
        "\n",
        "            num_examples = X.shape[0]\n",
        "\n",
        "            for i in range(num_examples):\n",
        "                x = np.expand_dims(np.transpose(X[i, :]), axis=1)\n",
        "                h = x.copy()\n",
        "\n",
        "                for layer in self.layers:\n",
        "                    h = layer.forward(h)\n",
        "\n",
        "                self.y_pred.append(h)\n",
        "\n",
        "            self.y_pred = np.array(self.y_pred, ndmin=2)\n",
        "\n",
        "            return self.y_pred\n",
        "            \n",
        "\n",
        "    def fit(self, X, y, val_data=None, batch_size=32, epochs=10):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.val_data = val_data\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.history = {'loss': [], 'val_loss': []}\n",
        "\n",
        "        num_examples = X.shape[0]\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0\n",
        "            \n",
        "            for i in range(num_examples):\n",
        "                x = np.expand_dims(np.transpose(X[i, :]), axis=1)\n",
        "\n",
        "                # Feed forward\n",
        "                h = x.copy()\n",
        "                for layer in self.layers:\n",
        "                    h = layer.forward(h)\n",
        "                \n",
        "                training_loss = self.loss(self.y[i], h)\n",
        "                total_loss += training_loss\n",
        "\n",
        "                # Backpropagation\n",
        "                # Accumulate layer gradients\n",
        "                for l in reversed(range(len(self.layers))):\n",
        "                    \n",
        "                    if l > 0:\n",
        "                        if self.layers[l].is_output == True:\n",
        "                            self.layers[l].backward(self.y[i], weights_data=None)\n",
        "\n",
        "                        else:\n",
        "                            self.layers[l].backward(self.layers[l + 1].delta, weights_data=self.layers[l + 1].weights)\n",
        "\n",
        "                        self.layers[l].grad += np.matmul(self.layers[l - 1].output, np.transpose(self.layers[l].delta))\n",
        "\n",
        "            # Get average and update weights\n",
        "            for l in reversed(range(len(self.layers))):\n",
        "                    if l > 0:\n",
        "                        self.layers[l].grad /= num_examples\n",
        "                        self.layers[l].weights -= self.learning_rate * self.layers[l].grad\n",
        "\n",
        "            mean_loss = total_loss / num_examples\n",
        "\n",
        "            if self.val_data == None:\n",
        "                print(\"Epoch\", epoch)\n",
        "                print(\"Training Loss:\", mean_loss)\n",
        "            else:\n",
        "                val_X = self.val_data[0]\n",
        "                val_y = self.val_data[1]\n",
        "                num_val_examples = val_X.shape[0]\n",
        "                total_val_loss = 0\n",
        "                for i in range(num_val_examples):\n",
        "                    val_x = np.expand_dims(np.transpose(val_X[i, :]), axis=1)\n",
        "                    val_h = val_x.copy()\n",
        "                    for layer in self.layers:\n",
        "                        val_h = layer.forward(val_h)\n",
        "                    val_loss = self.loss(val_y[i], h)\n",
        "                    total_val_loss += val_loss\n",
        "                mean_val_loss = total_val_loss / num_val_examples\n",
        "\n",
        "                print(\"Epoch\", epoch)\n",
        "                print(\"Training Loss:\", mean_loss, \"  Validation Loss:\", mean_val_loss)\n",
        "                self.history['loss'].append(mean_loss)\n",
        "                self.history['val_loss'].append(mean_val_loss)\n",
        "\n",
        "        self.trained = True\n",
        "        \n",
        "        self.history['loss'] = np.squeeze(self.history['loss'])\n",
        "        self.history['val_loss'] = np.squeeze(self.history['val_loss'])\n",
        "        \n",
        "        return self.history"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wwOFeboX5TT"
      },
      "source": [
        "# Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwnzJMbDWUqY"
      },
      "source": [
        "# y is true label value, h is prediction\n",
        "def binary_crossentropy(y, h):\n",
        "    return y * (-np.log(h)) + (1 - y) * (-np.log(1 - h))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FTF2OEkX_1M"
      },
      "source": [
        "# Activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36kuYoZgYJDn"
      },
      "source": [
        "def relu(z):\n",
        "    return z * (z > 0)\n",
        "\n",
        "# Cleaned up the relu gradient\n",
        "# Nice explanation of why its value at 0 doesn't matter:\n",
        "# https://www.quora.com/How-do-we-compute-the-gradient-of-a-ReLU-for-backpropagation\n",
        "def relu_grad(z):\n",
        "    return np.array(z > 0, dtype=np.int)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3rBlNsXYK5f"
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Should be (sigmoid(z)) * (1 - sigmoid(z)), rather than (sigmoid(z)) * (sigmoid(1 - z))\n",
        "def sigmoid_grad(z):\n",
        "    # return sigmoid(z) * sigmoid(1-z)\n",
        "    return sigmoid(z) * (1 - sigmoid(z))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40nHJ89AjCkK"
      },
      "source": [
        "def tanh(z):\n",
        "    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)\n",
        "\n",
        "def tanh_grad(z):\n",
        "    return 1 - (tanh(z) ** 2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-vAaYIh3DBP"
      },
      "source": [
        "# Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8tNlgRUVt6i"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nlINkKkVt64"
      },
      "source": [
        "data = pd.read_csv('/content/indian_liver_patient.csv')\n",
        "original_data = data.copy()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PvawODkVt7Y"
      },
      "source": [
        "## Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo0YTb3UVt7w"
      },
      "source": [
        "## Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvNhO1QYVt7x"
      },
      "source": [
        "def binary_encode(df, column, positive_value):\n",
        "    df = df.copy()\n",
        "    df[column] = df[column].apply(lambda x: 1 if x == positive_value else 0)\n",
        "    return df"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM0JucytVt7k"
      },
      "source": [
        "data['Albumin_and_Globulin_Ratio'] = data['Albumin_and_Globulin_Ratio'].fillna(data['Albumin_and_Globulin_Ratio'].mean())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VILmNxTUVt75"
      },
      "source": [
        "data = binary_encode(data, 'Gender', 'Male')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KVofsFLVt8C"
      },
      "source": [
        "Let's change the labels to 0, 1 instead of 1, 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GCNXwNOVt8D"
      },
      "source": [
        "data = binary_encode(data, 'Dataset', 1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqZXIz4N8xxT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9d0479d3-0e2d-4f6f-9f80-e83aeca8f537"
      },
      "source": [
        "data"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Total_Bilirubin</th>\n",
              "      <th>Direct_Bilirubin</th>\n",
              "      <th>Alkaline_Phosphotase</th>\n",
              "      <th>Alamine_Aminotransferase</th>\n",
              "      <th>Aspartate_Aminotransferase</th>\n",
              "      <th>Total_Protiens</th>\n",
              "      <th>Albumin</th>\n",
              "      <th>Albumin_and_Globulin_Ratio</th>\n",
              "      <th>Dataset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>65</td>\n",
              "      <td>0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.1</td>\n",
              "      <td>187</td>\n",
              "      <td>16</td>\n",
              "      <td>18</td>\n",
              "      <td>6.8</td>\n",
              "      <td>3.3</td>\n",
              "      <td>0.90</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>62</td>\n",
              "      <td>1</td>\n",
              "      <td>10.9</td>\n",
              "      <td>5.5</td>\n",
              "      <td>699</td>\n",
              "      <td>64</td>\n",
              "      <td>100</td>\n",
              "      <td>7.5</td>\n",
              "      <td>3.2</td>\n",
              "      <td>0.74</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>62</td>\n",
              "      <td>1</td>\n",
              "      <td>7.3</td>\n",
              "      <td>4.1</td>\n",
              "      <td>490</td>\n",
              "      <td>60</td>\n",
              "      <td>68</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.3</td>\n",
              "      <td>0.89</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>182</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>6.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>72</td>\n",
              "      <td>1</td>\n",
              "      <td>3.9</td>\n",
              "      <td>2.0</td>\n",
              "      <td>195</td>\n",
              "      <td>27</td>\n",
              "      <td>59</td>\n",
              "      <td>7.3</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0.40</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>578</th>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>500</td>\n",
              "      <td>20</td>\n",
              "      <td>34</td>\n",
              "      <td>5.9</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>579</th>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.1</td>\n",
              "      <td>98</td>\n",
              "      <td>35</td>\n",
              "      <td>31</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>580</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>245</td>\n",
              "      <td>48</td>\n",
              "      <td>49</td>\n",
              "      <td>6.4</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581</th>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>184</td>\n",
              "      <td>29</td>\n",
              "      <td>32</td>\n",
              "      <td>6.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>582</th>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>216</td>\n",
              "      <td>21</td>\n",
              "      <td>24</td>\n",
              "      <td>7.3</td>\n",
              "      <td>4.4</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>583 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Age  Gender  Total_Bilirubin  ...  Albumin  Albumin_and_Globulin_Ratio  Dataset\n",
              "0     65       0              0.7  ...      3.3                        0.90        1\n",
              "1     62       1             10.9  ...      3.2                        0.74        1\n",
              "2     62       1              7.3  ...      3.3                        0.89        1\n",
              "3     58       1              1.0  ...      3.4                        1.00        1\n",
              "4     72       1              3.9  ...      2.4                        0.40        1\n",
              "..   ...     ...              ...  ...      ...                         ...      ...\n",
              "578   60       1              0.5  ...      1.6                        0.37        0\n",
              "579   40       1              0.6  ...      3.2                        1.10        1\n",
              "580   52       1              0.8  ...      3.2                        1.00        1\n",
              "581   31       1              1.3  ...      3.4                        1.00        1\n",
              "582   38       1              1.0  ...      4.4                        1.50        0\n",
              "\n",
              "[583 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHpPLTz480jI"
      },
      "source": [
        "y = data['Dataset'].copy()\n",
        "X = data.drop('Dataset', axis=1).copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "y = np.expand_dims(np.array(y), axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqr1o4Xz3HYG"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXMgU2uG7pTP"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuKeUiKz7qlo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175fdc1a-77a1-4f57-bcd3-e6052f0b1d6e"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(326, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc0gZDgBF_UO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2114e3dd-86f0-47b9-e7dc-da6f44f95acb"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(326, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "468lyjwC7yji"
      },
      "source": [
        "model = NeuralNetwork(loss=binary_crossentropy, learning_rate=0.01)\n",
        "\n",
        "model.add(InputLayer(input_dim=10))\n",
        "model.add(DenseLayer(input_dim=10, output_dim=64, activation=relu, activation_grad=relu_grad))\n",
        "model.add(DenseLayer(input_dim=65, output_dim=64, activation=relu, activation_grad=relu_grad))\n",
        "model.add(DenseLayer(input_dim=65, output_dim=1, activation=sigmoid, activation_grad=sigmoid_grad))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MogZulxyohFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3adfc512-d05b-42d3-bc3f-79757c6a1477"
      },
      "source": [
        "for layer in model.layers:\n",
        "    try:\n",
        "        print(layer.weights.shape)\n",
        "        print(layer.is_output)\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 64)\n",
            "False\n",
            "(65, 64)\n",
            "False\n",
            "(65, 1)\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfQWQcs-3QXF"
      },
      "source": [
        "val_data = (X_val, y_val)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N0E7l4N-JSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa4289b7-af8a-4ec6-faa4-28143bed2b44"
      },
      "source": [
        "batch_size=32\n",
        "epochs = 200\n",
        "\n",
        "history = model.fit(X_train, y_train, val_data=val_data, batch_size=batch_size, epochs=epochs)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Training Loss: [[2.04875419]]   Validation Loss: [[2.67606518]]\n",
            "Epoch 1\n",
            "Training Loss: [[1.85369474]]   Validation Loss: [[2.42095568]]\n",
            "Epoch 2\n",
            "Training Loss: [[1.67756929]]   Validation Loss: [[2.1821444]]\n",
            "Epoch 3\n",
            "Training Loss: [[1.51959515]]   Validation Loss: [[1.96154604]]\n",
            "Epoch 4\n",
            "Training Loss: [[1.37875078]]   Validation Loss: [[1.75906682]]\n",
            "Epoch 5\n",
            "Training Loss: [[1.25405991]]   Validation Loss: [[1.57545782]]\n",
            "Epoch 6\n",
            "Training Loss: [[1.14567665]]   Validation Loss: [[1.41190626]]\n",
            "Epoch 7\n",
            "Training Loss: [[1.05316566]]   Validation Loss: [[1.26920337]]\n",
            "Epoch 8\n",
            "Training Loss: [[0.97625775]]   Validation Loss: [[1.14789321]]\n",
            "Epoch 9\n",
            "Training Loss: [[0.91440123]]   Validation Loss: [[1.04674582]]\n",
            "Epoch 10\n",
            "Training Loss: [[0.86649865]]   Validation Loss: [[0.96477337]]\n",
            "Epoch 11\n",
            "Training Loss: [[0.83042924]]   Validation Loss: [[0.89976204]]\n",
            "Epoch 12\n",
            "Training Loss: [[0.80346382]]   Validation Loss: [[0.84861684]]\n",
            "Epoch 13\n",
            "Training Loss: [[0.78320025]]   Validation Loss: [[0.80841729]]\n",
            "Epoch 14\n",
            "Training Loss: [[0.76762154]]   Validation Loss: [[0.77658512]]\n",
            "Epoch 15\n",
            "Training Loss: [[0.75528599]]   Validation Loss: [[0.75105137]]\n",
            "Epoch 16\n",
            "Training Loss: [[0.74520408]]   Validation Loss: [[0.73024467]]\n",
            "Epoch 17\n",
            "Training Loss: [[0.73672445]]   Validation Loss: [[0.71307005]]\n",
            "Epoch 18\n",
            "Training Loss: [[0.72939441]]   Validation Loss: [[0.69867995]]\n",
            "Epoch 19\n",
            "Training Loss: [[0.72292265]]   Validation Loss: [[0.6865083]]\n",
            "Epoch 20\n",
            "Training Loss: [[0.71711975]]   Validation Loss: [[0.67609512]]\n",
            "Epoch 21\n",
            "Training Loss: [[0.71185237]]   Validation Loss: [[0.66711906]]\n",
            "Epoch 22\n",
            "Training Loss: [[0.70700779]]   Validation Loss: [[0.65932665]]\n",
            "Epoch 23\n",
            "Training Loss: [[0.70250676]]   Validation Loss: [[0.65250496]]\n",
            "Epoch 24\n",
            "Training Loss: [[0.69831851]]   Validation Loss: [[0.64652512]]\n",
            "Epoch 25\n",
            "Training Loss: [[0.69438709]]   Validation Loss: [[0.64124195]]\n",
            "Epoch 26\n",
            "Training Loss: [[0.69067569]]   Validation Loss: [[0.63655229]]\n",
            "Epoch 27\n",
            "Training Loss: [[0.68718921]]   Validation Loss: [[0.63238052]]\n",
            "Epoch 28\n",
            "Training Loss: [[0.68387255]]   Validation Loss: [[0.62865692]]\n",
            "Epoch 29\n",
            "Training Loss: [[0.68071118]]   Validation Loss: [[0.62531867]]\n",
            "Epoch 30\n",
            "Training Loss: [[0.67769629]]   Validation Loss: [[0.6223146]]\n",
            "Epoch 31\n",
            "Training Loss: [[0.6748133]]   Validation Loss: [[0.61959507]]\n",
            "Epoch 32\n",
            "Training Loss: [[0.67204056]]   Validation Loss: [[0.61713587]]\n",
            "Epoch 33\n",
            "Training Loss: [[0.6693611]]   Validation Loss: [[0.61490384]]\n",
            "Epoch 34\n",
            "Training Loss: [[0.66678071]]   Validation Loss: [[0.61287659]]\n",
            "Epoch 35\n",
            "Training Loss: [[0.6642915]]   Validation Loss: [[0.61103238]]\n",
            "Epoch 36\n",
            "Training Loss: [[0.66188579]]   Validation Loss: [[0.60935231]]\n",
            "Epoch 37\n",
            "Training Loss: [[0.65955471]]   Validation Loss: [[0.60781704]]\n",
            "Epoch 38\n",
            "Training Loss: [[0.6572912]]   Validation Loss: [[0.6064169]]\n",
            "Epoch 39\n",
            "Training Loss: [[0.6550917]]   Validation Loss: [[0.60513495]]\n",
            "Epoch 40\n",
            "Training Loss: [[0.65295446]]   Validation Loss: [[0.60396546]]\n",
            "Epoch 41\n",
            "Training Loss: [[0.6508764]]   Validation Loss: [[0.60289487]]\n",
            "Epoch 42\n",
            "Training Loss: [[0.64885447]]   Validation Loss: [[0.60191434]]\n",
            "Epoch 43\n",
            "Training Loss: [[0.64688759]]   Validation Loss: [[0.60101704]]\n",
            "Epoch 44\n",
            "Training Loss: [[0.64496879]]   Validation Loss: [[0.60019635]]\n",
            "Epoch 45\n",
            "Training Loss: [[0.64309418]]   Validation Loss: [[0.59943802]]\n",
            "Epoch 46\n",
            "Training Loss: [[0.64126781]]   Validation Loss: [[0.59874446]]\n",
            "Epoch 47\n",
            "Training Loss: [[0.63948641]]   Validation Loss: [[0.59811019]]\n",
            "Epoch 48\n",
            "Training Loss: [[0.6377462]]   Validation Loss: [[0.59753133]]\n",
            "Epoch 49\n",
            "Training Loss: [[0.63604565]]   Validation Loss: [[0.59700333]]\n",
            "Epoch 50\n",
            "Training Loss: [[0.63438243]]   Validation Loss: [[0.59652378]]\n",
            "Epoch 51\n",
            "Training Loss: [[0.63275684]]   Validation Loss: [[0.59608957]]\n",
            "Epoch 52\n",
            "Training Loss: [[0.63116651]]   Validation Loss: [[0.59569778]]\n",
            "Epoch 53\n",
            "Training Loss: [[0.62961318]]   Validation Loss: [[0.59534541]]\n",
            "Epoch 54\n",
            "Training Loss: [[0.62809522]]   Validation Loss: [[0.59502947]]\n",
            "Epoch 55\n",
            "Training Loss: [[0.62660738]]   Validation Loss: [[0.59474843]]\n",
            "Epoch 56\n",
            "Training Loss: [[0.62514765]]   Validation Loss: [[0.59449908]]\n",
            "Epoch 57\n",
            "Training Loss: [[0.62371374]]   Validation Loss: [[0.59428035]]\n",
            "Epoch 58\n",
            "Training Loss: [[0.62230634]]   Validation Loss: [[0.59409008]]\n",
            "Epoch 59\n",
            "Training Loss: [[0.62092314]]   Validation Loss: [[0.59392626]]\n",
            "Epoch 60\n",
            "Training Loss: [[0.61956509]]   Validation Loss: [[0.59378738]]\n",
            "Epoch 61\n",
            "Training Loss: [[0.61823179]]   Validation Loss: [[0.59367247]]\n",
            "Epoch 62\n",
            "Training Loss: [[0.616925]]   Validation Loss: [[0.59358015]]\n",
            "Epoch 63\n",
            "Training Loss: [[0.61564416]]   Validation Loss: [[0.59350917]]\n",
            "Epoch 64\n",
            "Training Loss: [[0.61439163]]   Validation Loss: [[0.59345843]]\n",
            "Epoch 65\n",
            "Training Loss: [[0.61315999]]   Validation Loss: [[0.59342702]]\n",
            "Epoch 66\n",
            "Training Loss: [[0.61194899]]   Validation Loss: [[0.5934139]]\n",
            "Epoch 67\n",
            "Training Loss: [[0.61076117]]   Validation Loss: [[0.59341848]]\n",
            "Epoch 68\n",
            "Training Loss: [[0.60959319]]   Validation Loss: [[0.59343977]]\n",
            "Epoch 69\n",
            "Training Loss: [[0.60844894]]   Validation Loss: [[0.59347688]]\n",
            "Epoch 70\n",
            "Training Loss: [[0.60732495]]   Validation Loss: [[0.59352925]]\n",
            "Epoch 71\n",
            "Training Loss: [[0.60621682]]   Validation Loss: [[0.59359587]]\n",
            "Epoch 72\n",
            "Training Loss: [[0.60512549]]   Validation Loss: [[0.5936762]]\n",
            "Epoch 73\n",
            "Training Loss: [[0.60404881]]   Validation Loss: [[0.59376957]]\n",
            "Epoch 74\n",
            "Training Loss: [[0.60298661]]   Validation Loss: [[0.59387532]]\n",
            "Epoch 75\n",
            "Training Loss: [[0.60194169]]   Validation Loss: [[0.59399301]]\n",
            "Epoch 76\n",
            "Training Loss: [[0.60091116]]   Validation Loss: [[0.59412198]]\n",
            "Epoch 77\n",
            "Training Loss: [[0.59989356]]   Validation Loss: [[0.59426008]]\n",
            "Epoch 78\n",
            "Training Loss: [[0.59888789]]   Validation Loss: [[0.59440801]]\n",
            "Epoch 79\n",
            "Training Loss: [[0.59789822]]   Validation Loss: [[0.5945646]]\n",
            "Epoch 80\n",
            "Training Loss: [[0.59693076]]   Validation Loss: [[0.5947305]]\n",
            "Epoch 81\n",
            "Training Loss: [[0.59597837]]   Validation Loss: [[0.59490425]]\n",
            "Epoch 82\n",
            "Training Loss: [[0.59503986]]   Validation Loss: [[0.59508742]]\n",
            "Epoch 83\n",
            "Training Loss: [[0.59411463]]   Validation Loss: [[0.59528287]]\n",
            "Epoch 84\n",
            "Training Loss: [[0.5932025]]   Validation Loss: [[0.59548602]]\n",
            "Epoch 85\n",
            "Training Loss: [[0.59230257]]   Validation Loss: [[0.59569745]]\n",
            "Epoch 86\n",
            "Training Loss: [[0.59141522]]   Validation Loss: [[0.59591568]]\n",
            "Epoch 87\n",
            "Training Loss: [[0.5905414]]   Validation Loss: [[0.59614138]]\n",
            "Epoch 88\n",
            "Training Loss: [[0.58967866]]   Validation Loss: [[0.59637351]]\n",
            "Epoch 89\n",
            "Training Loss: [[0.5888276]]   Validation Loss: [[0.59661215]]\n",
            "Epoch 90\n",
            "Training Loss: [[0.58798788]]   Validation Loss: [[0.59685481]]\n",
            "Epoch 91\n",
            "Training Loss: [[0.58715843]]   Validation Loss: [[0.59710321]]\n",
            "Epoch 92\n",
            "Training Loss: [[0.58633981]]   Validation Loss: [[0.59735688]]\n",
            "Epoch 93\n",
            "Training Loss: [[0.58553414]]   Validation Loss: [[0.59761652]]\n",
            "Epoch 94\n",
            "Training Loss: [[0.58473911]]   Validation Loss: [[0.5978735]]\n",
            "Epoch 95\n",
            "Training Loss: [[0.58395318]]   Validation Loss: [[0.59813506]]\n",
            "Epoch 96\n",
            "Training Loss: [[0.58317675]]   Validation Loss: [[0.59840075]]\n",
            "Epoch 97\n",
            "Training Loss: [[0.58240918]]   Validation Loss: [[0.59867042]]\n",
            "Epoch 98\n",
            "Training Loss: [[0.58164691]]   Validation Loss: [[0.59894286]]\n",
            "Epoch 99\n",
            "Training Loss: [[0.58089322]]   Validation Loss: [[0.59921852]]\n",
            "Epoch 100\n",
            "Training Loss: [[0.58014635]]   Validation Loss: [[0.59949846]]\n",
            "Epoch 101\n",
            "Training Loss: [[0.57940872]]   Validation Loss: [[0.59978197]]\n",
            "Epoch 102\n",
            "Training Loss: [[0.57867905]]   Validation Loss: [[0.60006873]]\n",
            "Epoch 103\n",
            "Training Loss: [[0.57795974]]   Validation Loss: [[0.60035763]]\n",
            "Epoch 104\n",
            "Training Loss: [[0.57725007]]   Validation Loss: [[0.6006405]]\n",
            "Epoch 105\n",
            "Training Loss: [[0.57654909]]   Validation Loss: [[0.60092608]]\n",
            "Epoch 106\n",
            "Training Loss: [[0.57585622]]   Validation Loss: [[0.60121166]]\n",
            "Epoch 107\n",
            "Training Loss: [[0.57517124]]   Validation Loss: [[0.60149894]]\n",
            "Epoch 108\n",
            "Training Loss: [[0.57449399]]   Validation Loss: [[0.60178948]]\n",
            "Epoch 109\n",
            "Training Loss: [[0.57382414]]   Validation Loss: [[0.60208198]]\n",
            "Epoch 110\n",
            "Training Loss: [[0.57315748]]   Validation Loss: [[0.60238012]]\n",
            "Epoch 111\n",
            "Training Loss: [[0.57249528]]   Validation Loss: [[0.60268055]]\n",
            "Epoch 112\n",
            "Training Loss: [[0.57183803]]   Validation Loss: [[0.60298207]]\n",
            "Epoch 113\n",
            "Training Loss: [[0.57118686]]   Validation Loss: [[0.60328472]]\n",
            "Epoch 114\n",
            "Training Loss: [[0.57054255]]   Validation Loss: [[0.60358865]]\n",
            "Epoch 115\n",
            "Training Loss: [[0.56990511]]   Validation Loss: [[0.60389272]]\n",
            "Epoch 116\n",
            "Training Loss: [[0.5692728]]   Validation Loss: [[0.60419847]]\n",
            "Epoch 117\n",
            "Training Loss: [[0.56864504]]   Validation Loss: [[0.60450674]]\n",
            "Epoch 118\n",
            "Training Loss: [[0.56802464]]   Validation Loss: [[0.60481753]]\n",
            "Epoch 119\n",
            "Training Loss: [[0.56740788]]   Validation Loss: [[0.60512703]]\n",
            "Epoch 120\n",
            "Training Loss: [[0.56679834]]   Validation Loss: [[0.60544023]]\n",
            "Epoch 121\n",
            "Training Loss: [[0.56619525]]   Validation Loss: [[0.60575378]]\n",
            "Epoch 122\n",
            "Training Loss: [[0.56560096]]   Validation Loss: [[0.60606816]]\n",
            "Epoch 123\n",
            "Training Loss: [[0.5650109]]   Validation Loss: [[0.60638487]]\n",
            "Epoch 124\n",
            "Training Loss: [[0.56442517]]   Validation Loss: [[0.60670478]]\n",
            "Epoch 125\n",
            "Training Loss: [[0.56384654]]   Validation Loss: [[0.60702532]]\n",
            "Epoch 126\n",
            "Training Loss: [[0.56327295]]   Validation Loss: [[0.60734728]]\n",
            "Epoch 127\n",
            "Training Loss: [[0.56270424]]   Validation Loss: [[0.60766979]]\n",
            "Epoch 128\n",
            "Training Loss: [[0.56214196]]   Validation Loss: [[0.60799159]]\n",
            "Epoch 129\n",
            "Training Loss: [[0.56158566]]   Validation Loss: [[0.60831374]]\n",
            "Epoch 130\n",
            "Training Loss: [[0.56103508]]   Validation Loss: [[0.60863636]]\n",
            "Epoch 131\n",
            "Training Loss: [[0.56048905]]   Validation Loss: [[0.60895838]]\n",
            "Epoch 132\n",
            "Training Loss: [[0.55994859]]   Validation Loss: [[0.60928024]]\n",
            "Epoch 133\n",
            "Training Loss: [[0.55941126]]   Validation Loss: [[0.60960034]]\n",
            "Epoch 134\n",
            "Training Loss: [[0.55887831]]   Validation Loss: [[0.60991994]]\n",
            "Epoch 135\n",
            "Training Loss: [[0.55834959]]   Validation Loss: [[0.61023928]]\n",
            "Epoch 136\n",
            "Training Loss: [[0.55782566]]   Validation Loss: [[0.61055845]]\n",
            "Epoch 137\n",
            "Training Loss: [[0.55730853]]   Validation Loss: [[0.61087898]]\n",
            "Epoch 138\n",
            "Training Loss: [[0.5567992]]   Validation Loss: [[0.61120384]]\n",
            "Epoch 139\n",
            "Training Loss: [[0.55629423]]   Validation Loss: [[0.61152795]]\n",
            "Epoch 140\n",
            "Training Loss: [[0.55579546]]   Validation Loss: [[0.6118501]]\n",
            "Epoch 141\n",
            "Training Loss: [[0.55530206]]   Validation Loss: [[0.6121747]]\n",
            "Epoch 142\n",
            "Training Loss: [[0.5548116]]   Validation Loss: [[0.61249829]]\n",
            "Epoch 143\n",
            "Training Loss: [[0.55432557]]   Validation Loss: [[0.61282118]]\n",
            "Epoch 144\n",
            "Training Loss: [[0.55384427]]   Validation Loss: [[0.6131425]]\n",
            "Epoch 145\n",
            "Training Loss: [[0.55336778]]   Validation Loss: [[0.61346409]]\n",
            "Epoch 146\n",
            "Training Loss: [[0.55289544]]   Validation Loss: [[0.61378353]]\n",
            "Epoch 147\n",
            "Training Loss: [[0.55242759]]   Validation Loss: [[0.61410318]]\n",
            "Epoch 148\n",
            "Training Loss: [[0.55196464]]   Validation Loss: [[0.61442815]]\n",
            "Epoch 149\n",
            "Training Loss: [[0.55150459]]   Validation Loss: [[0.61475264]]\n",
            "Epoch 150\n",
            "Training Loss: [[0.55104899]]   Validation Loss: [[0.61507655]]\n",
            "Epoch 151\n",
            "Training Loss: [[0.55059705]]   Validation Loss: [[0.61539852]]\n",
            "Epoch 152\n",
            "Training Loss: [[0.55014923]]   Validation Loss: [[0.61571853]]\n",
            "Epoch 153\n",
            "Training Loss: [[0.54970463]]   Validation Loss: [[0.61604498]]\n",
            "Epoch 154\n",
            "Training Loss: [[0.54926371]]   Validation Loss: [[0.61636572]]\n",
            "Epoch 155\n",
            "Training Loss: [[0.54882734]]   Validation Loss: [[0.61668746]]\n",
            "Epoch 156\n",
            "Training Loss: [[0.54839684]]   Validation Loss: [[0.6170023]]\n",
            "Epoch 157\n",
            "Training Loss: [[0.54797248]]   Validation Loss: [[0.61733462]]\n",
            "Epoch 158\n",
            "Training Loss: [[0.54755388]]   Validation Loss: [[0.61765612]]\n",
            "Epoch 159\n",
            "Training Loss: [[0.54713934]]   Validation Loss: [[0.61798479]]\n",
            "Epoch 160\n",
            "Training Loss: [[0.54672644]]   Validation Loss: [[0.6183101]]\n",
            "Epoch 161\n",
            "Training Loss: [[0.54631732]]   Validation Loss: [[0.61863526]]\n",
            "Epoch 162\n",
            "Training Loss: [[0.54591146]]   Validation Loss: [[0.61895856]]\n",
            "Epoch 163\n",
            "Training Loss: [[0.54550907]]   Validation Loss: [[0.61927968]]\n",
            "Epoch 164\n",
            "Training Loss: [[0.54510951]]   Validation Loss: [[0.61959944]]\n",
            "Epoch 165\n",
            "Training Loss: [[0.54471271]]   Validation Loss: [[0.61991795]]\n",
            "Epoch 166\n",
            "Training Loss: [[0.54431957]]   Validation Loss: [[0.62023542]]\n",
            "Epoch 167\n",
            "Training Loss: [[0.54393292]]   Validation Loss: [[0.62054713]]\n",
            "Epoch 168\n",
            "Training Loss: [[0.54355048]]   Validation Loss: [[0.62085831]]\n",
            "Epoch 169\n",
            "Training Loss: [[0.54317087]]   Validation Loss: [[0.62116703]]\n",
            "Epoch 170\n",
            "Training Loss: [[0.54279375]]   Validation Loss: [[0.62147607]]\n",
            "Epoch 171\n",
            "Training Loss: [[0.54241851]]   Validation Loss: [[0.62178387]]\n",
            "Epoch 172\n",
            "Training Loss: [[0.54204534]]   Validation Loss: [[0.62208964]]\n",
            "Epoch 173\n",
            "Training Loss: [[0.54167366]]   Validation Loss: [[0.62239312]]\n",
            "Epoch 174\n",
            "Training Loss: [[0.54130409]]   Validation Loss: [[0.62269553]]\n",
            "Epoch 175\n",
            "Training Loss: [[0.5409383]]   Validation Loss: [[0.62299182]]\n",
            "Epoch 176\n",
            "Training Loss: [[0.54057526]]   Validation Loss: [[0.62328902]]\n",
            "Epoch 177\n",
            "Training Loss: [[0.54021521]]   Validation Loss: [[0.62358274]]\n",
            "Epoch 178\n",
            "Training Loss: [[0.53986089]]   Validation Loss: [[0.62388443]]\n",
            "Epoch 179\n",
            "Training Loss: [[0.53950905]]   Validation Loss: [[0.62418869]]\n",
            "Epoch 180\n",
            "Training Loss: [[0.53915881]]   Validation Loss: [[0.62448879]]\n",
            "Epoch 181\n",
            "Training Loss: [[0.53881057]]   Validation Loss: [[0.62478781]]\n",
            "Epoch 182\n",
            "Training Loss: [[0.53846413]]   Validation Loss: [[0.62508866]]\n",
            "Epoch 183\n",
            "Training Loss: [[0.53811903]]   Validation Loss: [[0.62538584]]\n",
            "Epoch 184\n",
            "Training Loss: [[0.53777526]]   Validation Loss: [[0.6256798]]\n",
            "Epoch 185\n",
            "Training Loss: [[0.53743493]]   Validation Loss: [[0.62598319]]\n",
            "Epoch 186\n",
            "Training Loss: [[0.537097]]   Validation Loss: [[0.62628064]]\n",
            "Epoch 187\n",
            "Training Loss: [[0.53676163]]   Validation Loss: [[0.62658104]]\n",
            "Epoch 188\n",
            "Training Loss: [[0.53642836]]   Validation Loss: [[0.6268781]]\n",
            "Epoch 189\n",
            "Training Loss: [[0.5360986]]   Validation Loss: [[0.62717621]]\n",
            "Epoch 190\n",
            "Training Loss: [[0.53577344]]   Validation Loss: [[0.62746961]]\n",
            "Epoch 191\n",
            "Training Loss: [[0.53545175]]   Validation Loss: [[0.62776408]]\n",
            "Epoch 192\n",
            "Training Loss: [[0.53513222]]   Validation Loss: [[0.6280565]]\n",
            "Epoch 193\n",
            "Training Loss: [[0.53481478]]   Validation Loss: [[0.6283513]]\n",
            "Epoch 194\n",
            "Training Loss: [[0.53449944]]   Validation Loss: [[0.62864336]]\n",
            "Epoch 195\n",
            "Training Loss: [[0.53418554]]   Validation Loss: [[0.62893493]]\n",
            "Epoch 196\n",
            "Training Loss: [[0.53387352]]   Validation Loss: [[0.62922482]]\n",
            "Epoch 197\n",
            "Training Loss: [[0.5335648]]   Validation Loss: [[0.62952148]]\n",
            "Epoch 198\n",
            "Training Loss: [[0.53325748]]   Validation Loss: [[0.62981414]]\n",
            "Epoch 199\n",
            "Training Loss: [[0.53295229]]   Validation Loss: [[0.63010663]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkepdEZ45vLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5496863-e973-40af-ec19-e18884a7c8fb"
      },
      "source": [
        "np.argmin(history['val_loss'])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcNh5dpQgaVc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "bc9f80ef-33a4-4ae7-9b41-5ae12ad78f48"
      },
      "source": [
        "epochs_range = range(epochs)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.plot(\n",
        "    epochs_range,\n",
        "    history['loss'],\n",
        "    label=\"Training Loss\"\n",
        ")\n",
        "\n",
        "plt.plot(\n",
        "    epochs_range,\n",
        "    history['val_loss'],\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Loss Over Time\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlPfOMdxmEnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3782af47-293f-4356-d253-c87972d2e899"
      },
      "source": [
        "np.argmin(history['val_loss'])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwOOA3v11ri2"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.array(y_pred > 0.5, dtype=np.int)\n",
        "\n",
        "y_pred = np.squeeze(y_pred)\n",
        "y_true = np.squeeze(y_test)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFzb081d15IQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd1c80a-113a-4eb1-fcb8-03b59097d91d"
      },
      "source": [
        "# Getting accuracy\n",
        "\n",
        "results = (y_true == y_pred)\n",
        "print(np.mean(results))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.72\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}